{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project Delivery Report: Lips Reading\n",
    "## Motivation\n",
    "  * Potential applications:\n",
    "    * Benefitial earing aids.\n",
    "    * Outdoor communication with AR glasses.\n",
    "    * Large dataset can be collected from online resources.\n",
    "    * Utilize what we learned from this module like cnn/rnn\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Dataset\n",
    "## Description\n",
    "  * Number of training videos: 171\n",
    "  * Number of validation videos: 20\n",
    "  * Maximun alphabets of each video: 5\n",
    "  * Classes: 27 (includes 26 letters, blank and EOS flag)\n",
    "  * Number of person :7"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "    \n",
    "## Preprocess method\n",
    "\n",
    "\n",
    "Our team recorded 191 raw video clips. In order to give more constraint for the nerual network, we used a face detection model to preprocess the raw frames. The steps are shown in the above pipeline. Firstly, we will detect the five landmarks, and then followed by an affine transform operation(cv2.warpAffine()) to make sure that the mounth is at a consistent position.\n",
    "![preprocess.png](./report_img/preprocess.png)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Data Statistics\n",
    "![download.png](./report_img/download.png)\n",
    "\n",
    "The first graph shows the our training dataset distribution of label sequence length . The second graph tells the the 26 alphabet number distribution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network Design\n",
    "\n",
    "More to cover\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## MLP + LSTM\n",
    "Our input is a sequence of images with length of 200. In order to use the multi layer Perceptron(MLP), we skip 4 frames and select the fifth instead of using all frames which resulted in out of memory issue.  In this experiment, we use six linear layers and followed by an relu activation layer to encode the frames and reshape the output to target sequence length, which is six here. Bidirectional LSTM module is applied to decode the input features predict class of 26 alphabets and 2 flags.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample Input and outout"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "! python mlp/mlp_Generator.py\n",
    "\n",
    "! python mlp/mlp_model.py"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'<blank>': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '<eos>': 27}\n",
      "train videos:171\n",
      "test videos:20\n",
      "Input shape: torch.Size([5, 432000])\n",
      "Label shape: torch.Size([5, 6])\n",
      "Label: tensor([[12, 13, 14, 15, 27,  0],\n",
      "        [ 5,  1, 27,  0,  0,  0],\n",
      "        [18, 18, 22, 26, 27,  0],\n",
      "        [19,  4,  5, 27,  0,  0],\n",
      "        [ 6, 15, 24, 27,  0,  0]])\n",
      "Output shape: torch.Size([5, 6, 28])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "![3DCNN+LSTM](./report_img/MLP+LSTM.png)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## CNN + LSTM\n",
    "\n",
    "![3DCNN+LSTM](./report_img/2DCNN+LSTM.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 3DCNN + LSTM\n",
    "![3DCNN+LSTM](./report_img/3DCNN+LSTM.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Result\n",
    "\n",
    "\n",
    "structure | Train loss | val loss | val acc|\n",
    "--------- | ---------- | -------- | -------|\n",
    "MLP + LSTM| 2.11| 2.61|??|\n",
    "CNN + LSTM | ?  |?    |? |\n",
    "3DCNN + LSTM | ?  |?    |? |\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference visualize"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "331356ca85e702178c703a299ac9d9de723919524fe12d97979a0dacce9e25f6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}