{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project Delivery Report: Lips Reading\n",
    "## Motivation\n",
    "\n",
    "Deep learning in multi-modality has achieved significant progress in these years, and some of them have applied in\n",
    "our daily life like audio-to-text, text-to-audio and text to images. Lipreading is one of the challenging tasks which\n",
    "involves decoding features from the movement from speakers’ mouths or faces. Many potential applications can be\n",
    "benefited from lipreading like hearing aids, speech recognition in noisy environments, outdoor communication with\n",
    "AR glasses. Humans’ performance is not satisfied with this task. By leveraging the online video and text resources,\n",
    "lipreading with deep learning might have more promising results than human. Instead of doing classification tasks in\n",
    "a single frame, Lipreading needs to extract spatio-temporal features from video sequences, which increase our\n",
    "project difficulty.\n",
    "\n",
    "  * Potential applications:\n",
    "    * Benefitial earing aids.\n",
    "    * Outdoor communication with AR glasses.\n",
    "    * Large dataset can be collected from online resources.\n",
    "    * Utilize what we learned from this module like cnn/rnn\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "In this task, the input is a video or a series of image frames, which contain the visual features of speakers’ mouths or\n",
    "faces, while the output is a sequence of text. So the core target of this project is to recognize a sequence of characters\n",
    "from a sequence of images (seq2seq task). Without loss of generality, we assume that the input videos will only\n",
    "contain the lip movements of reading `alphabet characters (a, b, c, d, e,..., z)` in random order and our target is to employ proposed\n",
    "methods to recognize these corresponding `alphabet characters (a, b, c, d, e,..., z)`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Dataset\n",
    "This is not a public dataset, we created this lips reading dataset by ourselves.  \n",
    "\n",
    "### How did we generate this dataset?  \n",
    "  * we invited 7 people (including our team members) to record their alphabet pronunciation videos by their phones\n",
    "  \n",
    "### Details\n",
    "  * Number of training videos: 171\n",
    "  * Number of validation videos: 20\n",
    "  * Maximun alphabet character length in this dataset: 5\n",
    "  * Character classes: 28 (includes 26 alphabet letters (a, b, c, d, e, f,..., z), blank and eos flag)\n",
    "  * Number of lips-reading recording participants: 7"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proposed Methods\n",
    "\n",
    "Datasets contain multiple images (different frames of the video) per data instance, i.e. the X in an (X, Y) pair in the\n",
    "dataset, is not a single image but a list of images with variable length.  \n",
    "\n",
    "* **2D CNN + RNN**: A series of frames can be concatenated into a larger 2D grid image. Our 2D CNN model will extract the visual features of lip movements from this grid frame and the following RNN model will recognize and decode the corresponding characters of this video.\n",
    "* **3D CNN + RNN**: We can employ 3D CNN to encode the spatio-temporal features from video sequences directly. Then these features will be fed into RNN models to decode the output character sequences\n",
    "* **MLP + RNN**: Same idea, but the video is flatten and fed into MLP layers to extract the visual features. RNN works as a target character decoder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "    \n",
    "## Preprocess method\n",
    "\n",
    "\n",
    "Our team recorded 191 raw video clips. In order to give more constraint for the nerual network, we used a face detection model to preprocess the raw frames. The steps are shown in the above pipeline. Firstly, we will detect the five landmarks, and then followed by an affine transform operation(cv2.warpAffine()) to make sure that the mounth is at a consistent position.\n",
    "![preprocess.png](report_img/preprocess.png)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Data Statistics\n",
    "![download.png](report_img/download.png)\n",
    "The first graph shows the our training dataset distribution of label sequence length . The second graph tells the the 26 alphabet number distribution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Network Design\n",
    "In this part, we present the detailed network design and model architectures of our proposed methods\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### MLP + LSTM\n",
    "\n",
    "! python mlp/mlp_model.py"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "![3DCNN+LSTM](report_img/MLP+LSTM.png)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### CNN + LSTM\n",
    "\n",
    "![3DCNN+LSTM](./report_img/2DCNN+LSTM.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 3DCNN + LSTM\n",
    "![3DCNN+LSTM](report_img/3DCNN+LSTM.png)\n",
    "\n",
    "![3DCNN+LSTM+Details](report_img/3DCNN_RNN_model.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Inference\n",
    "### [training and inference of 3DCNN_RNN model](3DCNN_RNN_lipsreading.ipynb)\n",
    "### [training and inference of MLP_RNN model](MLP_RNN_lipsreading.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Peformance\n",
    "\n",
    "\n",
    "structure | Train loss | val loss | val acc|\n",
    "--------- | ---------- | -------- | -------|\n",
    "MLP + LSTM| 2.24| 2.61|0.31|\n",
    "CNN + LSTM | ?  |?    |? |\n",
    "3DCNN + LSTM | 1.90  |2.25 |0.67 |\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "331356ca85e702178c703a299ac9d9de723919524fe12d97979a0dacce9e25f6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}