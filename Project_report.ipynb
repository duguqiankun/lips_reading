{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Delivery Report: Lips Reading\n",
    "## Motivation\n",
    "\n",
    "Deep learning in multi-modality has achieved significant progress in these years, and some of them have applied in\n",
    "our daily life like audio-to-text, text-to-audio and text to images. Lipreading is one of the challenging tasks which\n",
    "involves decoding features from the movement from speakers’ mouths or faces. Many potential applications can be\n",
    "benefited from lipreading like hearing aids, speech recognition in noisy environments, outdoor communication with\n",
    "AR glasses. Humans’ performance is not satisfied with this task. By leveraging the online video and text resources,\n",
    "lipreading with deep learning might have more promising results than human. Instead of doing classification tasks in\n",
    "a single frame, Lipreading needs to extract spatio-temporal features from video sequences, which increase our\n",
    "project difficulty.\n",
    "\n",
    "  * Potential applications:\n",
    "    * Benefitial earing aids.\n",
    "    * Outdoor communication with AR glasses.\n",
    "    * Large dataset can be collected from online resources.\n",
    "    * Utilize what we learned from this module like cnn/rnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this task, the input is a video or a series of image frames, which contain the visual features of speakers’ mouths or\n",
    "faces, while the output is a sequence of text. So the core target of this project is to recognize a sequence of characters\n",
    "from a sequence of images (seq2seq task). Without loss of generality, we assume that the input videos will only\n",
    "contain the lip movements of reading `alphabet characters (a, b, c, d, e,..., z)` in random order and our target is to employ proposed\n",
    "methods to recognize these corresponding `alphabet characters (a, b, c, d, e,..., z)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Dataset\n",
    "This is not a public dataset, we created this lips reading dataset by ourselves.  \n",
    "\n",
    "### How did we generate this dataset?  \n",
    "  * we invited 7 people (including our team members) to record their alphabet pronunciation videos by their phones\n",
    "  \n",
    "### Details\n",
    "  * Number of training videos: 171\n",
    "  * Number of validation videos: 20\n",
    "  * Maximun alphabet character length in this dataset: 5\n",
    "  * Character classes: 28 (includes 26 alphabet letters (a, b, c, d, e, f,..., z), blank and eos flag)\n",
    "  * Number of lips-reading recording participants: 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Methods\n",
    "\n",
    "Datasets contain multiple images (different frames of the video) per data instance, i.e. the X in an (X, Y) pair in the\n",
    "dataset, is not a single image but a list of images with variable length.  \n",
    "\n",
    "* **2D CNN + RNN**: A series of frames can be concatenated into a larger 2D grid image. Our 2D CNN model will extract the visual features of lip movements from this grid frame and the following RNN model will recognize and decode the corresponding characters of this video.\n",
    "* **3D CNN + RNN**: We can employ 3D CNN to encode the spatio-temporal features from video sequences directly. Then these features will be fed into RNN models to decode the output character sequences\n",
    "* **MLP + RNN**: Same idea, but the video is flatten and fed into MLP layers to extract the visual features. RNN works as a target character decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## Preprocess method\n",
    "\n",
    "\n",
    "Our team recorded 191 raw video clips. In order to give more constraint for the nerual network, we used a face detection model to preprocess the raw frames. The steps are shown in the above pipeline. Firstly, we will detect the five landmarks, and then followed by an affine transform operation(cv2.warpAffine()) to make sure that the mounth is at a consistent position.\n",
    "![preprocess.png](report_img/preprocess.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Statistics\n",
    "![download.png](report_img/download.png)\n",
    "The first graph shows the our training dataset distribution of label sequence length . The second graph tells the the 26 alphabet number distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Design\n",
    "In this part, we present the detailed network design and model architectures of our proposed methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### MLP + LSTM\n",
    "\n",
    "! python mlp/mlp_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![3DCNN+LSTM](report_img/MLP+LSTM.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### CNN + LSTM\n",
    "\n",
    "![3DCNN+LSTM](./report_img/2DCNN+LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3DCNN + LSTM\n",
    "![3DCNN+LSTM](report_img/3DCNN+LSTM.png)\n",
    "\n",
    "![3DCNN+LSTM+Details](report_img/3DCNN_RNN_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Inference\n",
    "### [training and inference of 3DCNN_RNN model](3DCNN_RNN_lipsreading.ipynb)\n",
    "### [training and inference of 2DCNN_RNN model](2DCNN_RNN_lipsreading.ipynb)\n",
    "### [training and inference of MLP_RNN model](MLP_RNN_lipsreading.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peformance\n",
    "\n",
    "\n",
    "structure | Train loss | val loss | val acc|\n",
    "--------- | ---------- | -------- | -------|\n",
    "MLP + BiLSTM| 2.24| 2.61|0.31|\n",
    "2DCNN + BiLSTM with 5x5 images | 2.18  |2.51    |0.29 |\n",
    "2DCNN + BiLSTM with 8x8 images | 2.18  |2.47    |0.31 |\n",
    "3DCNN + BiLSTM | 1.90  |2.25 |0.67 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this lips reading project, we employ MLP/2D CNN/3D CNN as our visual encoder to extract lips movement features, and a basic BiLSTM layer is utilized to decode these visual features and predict the target character sequences. \n",
    "\n",
    "From the performance table, we compare various encoder models and can figure out that 3D CNN is the best encoder to learn the lips movement features of videos. However, when videos are flatten into one dimentional matrix or 2 dimentional concatenated large images, the local features in image and patio-temporal features in videos are damaged, thus the final performance of 2DCNN-RNN and MLP-RNN is not as good as 3DCNN-RNN. RNN models work as sequence decoder, which can transform the lips movement into corresponding target alphabet characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Development\n",
    "\n",
    "### Explore general lips reading task\n",
    "In our current task, we assume that our videos only contain the alphabet characters, which is sort of ideal scenario. In the future, we can explore the lips recognition task of words and sentences, instead of alphahet characters in videos.\n",
    "### Enlarge the dataset\n",
    "1. download more videos from open-source website.\n",
    "2. generate longer target sequences, not limited to current max length of 5\n",
    "\n",
    "### Improve the model\n",
    "1. Use deep backbone models as encoder, such as, ResNet50, EfficientNetB0 - B7, etc.\n",
    "2. For 2DCNN + LSTM model, we can increase the amount of sample images(e.g. 10x10, 15x15).\n",
    "3. Learning from NLP, we can try the self-supervise learning to learn the model.\n",
    "4. Train models with multi-scale images and videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "331356ca85e702178c703a299ac9d9de723919524fe12d97979a0dacce9e25f6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}