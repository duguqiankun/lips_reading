{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Delivery Report: Lips Reading\n",
    "## Motivation\n",
    "  * Potential applications:\n",
    "    * Benefitial earing aids.\n",
    "    * Outdoor communication with AR glasses.\n",
    "    * Large dataset can be collected from online resources.\n",
    "    * Utilize what we learned from this module like cnn/rnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Dataset\n",
    "## Description\n",
    "  * Number of training videos: 171\n",
    "  * Number of validation videos: 20\n",
    "  * Maximun alphabets of each video: 5\n",
    "  * Classes: 27 (includes 26 letters, blank and EOS flag)\n",
    "  * Number of person :7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## Preprocess method\n",
    "\n",
    "\n",
    "Our team recorded 191 raw video clips. In order to give more constraint for the nerual network, we used a face detection model to preprocess the raw frames. The steps are shown in the above pipeline. Firstly, we will detect the five landmarks, and then followed by an affine transform operation(cv2.warpAffine()) to make sure that the mounth is at a consistent position.\n",
    "![preprocess.png](./report_img/preprocess.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Statistics\n",
    "![download.png](./report_img/download.png)\n",
    "The first graph shows the our training dataset distribution of label sequence length . The second graph tells the the 26 alphabet number distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Design\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## MLP + LSTM\n",
    "\n",
    "\n",
    "![3DCNN+LSTM](./report_img/MLP+LSTM.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CNN + LSTM\n",
    "\n",
    "More \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3DCNN + LSTM\n",
    "![3DCNN+LSTM](./report_img/3DCNN+LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "\n",
    "\n",
    "structure | Train loss | val loss | val acc|\n",
    "--------- | ---------- | -------- | -------|\n",
    "MLP + LSTM| 2.11| 2.61|??|\n",
    "CNN + LSTM | ?  |?    |? |\n",
    "3DCNN + LSTM | ?  |?    |? |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
